{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(datapath, dirname, filename):\n",
    "    data_dirpath = os.path.join(datapath, dirname)\n",
    "    if os.path.isdir(data_dirpath):\n",
    "        data_filepath = os.path.join(data_dirpath, filename)\n",
    "        \n",
    "        if os.path.isfile(data_filepath):\n",
    "            data = pd.read_csv(data_filepath)\n",
    "            print(\"Loaded %s Data\" %(dirname))\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            print(\"Couldn't find file: %s\" %(data_filepath))\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        print(\"Couldn't find dir: %s\" %(data_dirpath))\n",
    "        return None\n",
    "\n",
    "def load_data(datapath):\n",
    "    \"\"\"\n",
    "    Function to load all csv files\n",
    "    Input:  Path to Main Data dir\n",
    "    Output: Pandas dataframes for each csv\n",
    "    \"\"\"\n",
    "    \n",
    "    nrc_text_data = load_csv(datapath, 'Text', 'nrc.csv')\n",
    "    liwc_text_data = load_csv(datapath, 'Text', 'liwc.csv')\n",
    "    relation_data = load_csv(datapath, 'Relation', 'Relation.csv')\n",
    "    profile_data = load_csv(datapath, 'Profile', 'Profile.csv')\n",
    "    image_data = load_csv(datapath, 'Image', 'oxford_merged.csv')\n",
    "            \n",
    "    return nrc_text_data, liwc_text_data, relation_data, profile_data, image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Text Data\n",
      "Loaded Text Data\n",
      "Loaded Relation Data\n",
      "Loaded Profile Data\n",
      "Loaded Image Data\n"
     ]
    }
   ],
   "source": [
    "data = load_data('C:\\\\Users\\\\jonat\\\\Documents\\\\School\\\\DataScience\\\\Project\\\\Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for loading pickles\n",
    "pickle_in = open(\"combined_df_merged.pkl\",\"rb\")\n",
    "data_merged = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#various data loading and preprocessing \n",
    "\n",
    "\n",
    "\n",
    "data_emotional = data[0]\n",
    "data_text = data_text.rename(columns={0:'userId'})\n",
    "#data_merged.iloc[:,2] = data_merged.iloc[:,2].apply(lambda x: 0 if x < 25 else (1 if x < 35 else (2 if x < 50 else 3)))\n",
    "data_text.iloc[:,2] = data_text.iloc[:,2].apply(lambda x: 0 if x < 25 else (1 if x < 35 else (2 if x < 50 else 3)))\n",
    "\n",
    "#data_text = data_text.sort_values(by=[0])\n",
    "#data_merged = data_merged.sort_values(by=[0])\n",
    "#data_emotional = data_emotional.sort_values(by=['userId'])\n",
    "combined_data = pd.merge(data_text, data_emo, on='userId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age on combined LIWC/NRC\n",
    "emo_age = GradientBoostingClassifier(n_estimators=400,max_depth=5,learning_rate=0.01)\n",
    "split = 5\n",
    "kf = KFold(n_splits=split)\n",
    "result = []\n",
    "data = combined_data\n",
    "for train_index, test_index in kf.split(data):\n",
    "    c = 0\n",
    "    train = data.iloc[train_index,8:]\n",
    "    test = data.iloc[test_index,8:]\n",
    "    train_labels = data.iloc[train_index,2]\n",
    "    test_labels = data.iloc[test_index,2]\n",
    "    emo_age.fit(train, train_labels)\n",
    "    t = emo_age.predict(test)\n",
    "    for i in range(len(test_labels)):\n",
    "        if t[i] == test_labels.iloc[i]:\n",
    "            c+=1\n",
    "    result.append(c/len(test_labels))\n",
    "#print(result)\n",
    "#print(sum(result)/len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gender on facial \n",
    "emo_gen = GradientBoostingClassifier(n_estimators=400,max_depth=5,learning_rate=0.01)\n",
    "split = 5\n",
    "kf = KFold(n_splits=split)\n",
    "result = []\n",
    "data = data_merged\n",
    "for train_index, test_index in kf.split(data):\n",
    "    c = 0\n",
    "    train = data.iloc[train_index,3:]\n",
    "    test = data.iloc[test_index,3:]\n",
    "    train_labels = data.iloc[train_index,1]\n",
    "    test_labels = data.iloc[test_index,1]\n",
    "    emo_gen.fit(train, train_labels)\n",
    "    t = emo_gen.predict(test)\n",
    "    for i in range(len(test_labels)):\n",
    "        if t[i] == test_labels.iloc[i]:\n",
    "            c+=1\n",
    "    result.append(c/len(test_labels))\n",
    "#print(result)\n",
    "#print(sum(result)/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(emo_gen, open( \"gender_model.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient boost can be replaced with any of the necessaey functions\n",
    "#the function can also be passed in as an argument, but this wasn't done here\n",
    "def run_functions(j,k):\n",
    "    r = []\n",
    "    emo_age = GradientBoostingClassifier(n_estimators=400,max_depth=5,learning_rate=0.01)\n",
    "    split = 5\n",
    "    kf = KFold(n_splits=split)\n",
    "    pca = PCA(n_components=2)\n",
    "    count = 0\n",
    "    data = data_text\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        train = data.iloc[train_index,np.r_[j,k]]\n",
    "        test = data.iloc[test_index,np.r_[j,k]]\n",
    "        train_labels = data.iloc[train_index,2]\n",
    "        test_labels = data.iloc[test_index,2]\n",
    "        log.fit(train, train_labels)\n",
    "        t = log.predict(test)\n",
    "        c = 0\n",
    "        for i in range(len(test_labels)):\n",
    "            if t[i] == test_labels.iloc[i]:\n",
    "                c+=1\n",
    "        result.append(c/len(test_labels))\n",
    "    r.append(sum(result)/len(result))\n",
    "    print(\"{} and {} index is {}\".format(k,j,sum(result)/len(result)))    \n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "#runs a loop over a permutation of n different features \n",
    "results = []\n",
    "indices = []\n",
    "for i in range(8,len(data_text.columns)):\n",
    "    for j in range(8, len(data_text.columns)):\n",
    "        if i != j:\n",
    "            results.append(run_functions(i,j))\n",
    "            indices.append((i,j))\n",
    "    print(\"Finished one outer loop feature\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original code to combine dataframes - deprecated when an easier method was developped\n",
    "\n",
    "data_text['userId'] = data_text['userId'].astype('str')\n",
    "data_profile['userid'] = data_profile['userid'].astype('str')\n",
    "data_profile.set_index('userid')\n",
    "data_facial.set_index('userId')\n",
    "\n",
    "data_user = pd.DataFrame()\n",
    "data_tex = pd.DataFrame()\n",
    "d = []\n",
    "el = []\n",
    "for i in range(len(data_profile)):\n",
    "    for j in range(len(data_text)):\n",
    "        user = []\n",
    "        if data_profile.loc[i,'userid'] == data_text.loc[j,'userId']:\n",
    "            user.append(data_profile.loc[i,'userid'])\n",
    "            user.append(data_profile.loc[i,'gender'])\n",
    "            user.append(data_profile.loc[i,'age'])\n",
    "            user.append(data_profile.loc[i,'ope'])\n",
    "            user.append(data_profile.loc[i,'con'])\n",
    "            user.append(data_profile.loc[i,'ext'])\n",
    "            user.append(data_profile.loc[i,'agr'])\n",
    "            user.append(data_profile.loc[i,'neu'])\n",
    "            el.append(data_text.iloc[j,1:])\n",
    "            if len(user) != 0:\n",
    "                d.append(user)\n",
    "\n",
    "d1 = pd.DataFrame(d)\n",
    "d2 = pd.DataFrame(el)\n",
    "d1.reset_index(drop=True, inplace=True)\n",
    "d2.reset_index(drop=True, inplace=True)\n",
    "frames = [d1, d2]\n",
    "d3 = pd.concat(frames, axis=1)\n",
    "d3.head()\n",
    "d3.to_pickle(\"combined_df_text.pkl\")            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we tried PCA and kmeans initially but it didn;t work so well - code left for interest only\n",
    "class pca_kmeans():\n",
    "    def __init__(self,components, splits, data):\n",
    "        self.results = []\n",
    "        self.comp = components\n",
    "        self.splits = splits\n",
    "        self.data = data\n",
    "        \n",
    "    def run(self):\n",
    "        for i in range(self.comp):\n",
    "            self.kfold(i)\n",
    "            print(\"Component {} out of {} done\".format(i+1, self.comp))\n",
    "            print(self.results[i])\n",
    "        return self.results\n",
    "        \n",
    "    def kfold(self, num_comp):\n",
    "        kf = KFold(n_splits=self.splits)\n",
    "        temp = []\n",
    "        for train_index, test_index in kf.split(self.data):\n",
    "            self.train = data.iloc[train_index,:]\n",
    "            self.test = data.iloc[test_index,:]\n",
    "            self.pca(num_comp)\n",
    "            t = sum(self.kmeans())/test_index.shape[0]\n",
    "            if t < 0.5:\n",
    "                t = 1 -t\n",
    "            temp.append(t)\n",
    "        mean = sum(temp)/len(temp)\n",
    "        self.results.append(mean)\n",
    "        \n",
    "    def pca(self, num_comp):\n",
    "        pca = PCA(n_components = num_comp+1)\n",
    "        pca.fit(self.train)\n",
    "        self.f = pca.transform(self.train)\n",
    "        self.features = pca.transform(self.test)\n",
    "        \n",
    "    def kmeans(self):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=2, n_init=25).fit(self.f)\n",
    "        return kmeans.predict(self.features)\n",
    "        \n",
    "#result = a.run()\n",
    "#result_n = b.run()\n",
    "#print(\"The result for a is \",sum(result)/len(result))\n",
    "#print(\"The result for b is \", sum(result_n)/len(result_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
